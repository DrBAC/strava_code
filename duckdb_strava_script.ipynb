{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b5cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import duckdb\n",
    "from fitparse import FitFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b6b38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "DB_FILE = 'project_database.duckdb'\n",
    "ACTIVITIES_FILE = 'strava_data_dumps/STRAVA+export_8029714/activities.csv'\n",
    "FIT_FILES_FOLDER = 'strava_data_dumps/STRAVA+export_8029714/activities/'\n",
    "\n",
    "IMPORTANT_FIELDS = ['heart_rate', 'cadence', 'speed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3107adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect(DB_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0c75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def create_tables():\n",
    "    \"\"\"Create activities and fit_logs tables (DuckDB compatible).\"\"\"\n",
    "    con.execute(\"DROP TABLE IF EXISTS activities\")\n",
    "    con.execute(\"DROP TABLE IF EXISTS fit_logs\")\n",
    "\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE activities (\n",
    "            activity_id BIGINT,\n",
    "            name TEXT,\n",
    "            date TIMESTAMP,\n",
    "            distance DOUBLE,\n",
    "            duration DOUBLE,\n",
    "            filename TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE fit_logs (\n",
    "            log_id BIGINT,\n",
    "            activity_id BIGINT,\n",
    "            timestamp TIMESTAMP,\n",
    "            field_name TEXT,\n",
    "            field_value DOUBLE\n",
    "        )\n",
    "    \"\"\")\n",
    "    print(\"✅ Tables created fresh.\")\n",
    "\n",
    "def get_next_id(table_name, id_column):\n",
    "    \"\"\"Find the next available ID for a table.\"\"\"\n",
    "    result = con.execute(f\"SELECT MAX({id_column}) FROM {table_name}\").fetchone()[0]\n",
    "    return 1 if result is None else result + 1\n",
    "\n",
    "def bulk_insert_activities(csv_path):\n",
    "    \"\"\"Load activities from CSV, assign activity_ids, and bulk insert.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if 'Filename' not in df.columns:\n",
    "        raise ValueError(\"activities.csv must have a 'filename' column.\")\n",
    "\n",
    "    next_id = get_next_id('activities', 'activity_id')\n",
    "    df.insert(0, 'activity_id', range(next_id, next_id + len(df)))\n",
    "\n",
    "    df.to_sql('activities', con, if_exists='append', index=False)\n",
    "    print(f\"✅ Inserted {len(df)} activities starting at ID {next_id}\")\n",
    "\n",
    "def parse_fit_file(filepath):\n",
    "    \"\"\"Parse a .fit file into a list of records.\"\"\"\n",
    "    try:\n",
    "        fitfile = FitFile(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to parse {filepath}: {e}\")\n",
    "        return []\n",
    "\n",
    "    entries = []\n",
    "    for record in fitfile.get_messages('record'):\n",
    "        timestamp = None\n",
    "        metrics = []\n",
    "\n",
    "        for field in record:\n",
    "            if field.name == 'timestamp':\n",
    "                timestamp = field.value\n",
    "            elif field.name in IMPORTANT_FIELDS and field.value is not None:\n",
    "                metrics.append((field.name, field.value))\n",
    "\n",
    "        if timestamp:\n",
    "            for field_name, field_value in metrics:\n",
    "                entries.append({\n",
    "                    'timestamp': timestamp,\n",
    "                    'field_name': field_name,\n",
    "                    'field_value': field_value\n",
    "                })\n",
    "\n",
    "    return entries\n",
    "\n",
    "def build_all_fit_logs():\n",
    "    \"\"\"Parse all .fit files into one big DataFrame of logs.\"\"\"\n",
    "    activities_df = pd.read_sql(\"SELECT activity_id, filename FROM activities\", con)\n",
    "\n",
    "    all_logs = []\n",
    "\n",
    "    fit_files = glob.glob(os.path.join(FIT_FILES_FOLDER, '*.fit'))\n",
    "\n",
    "    for filepath in fit_files:\n",
    "        filename = os.path.basename(filepath)\n",
    "        match = activities_df[activities_df['Filename'] == filename]\n",
    "\n",
    "        if match.empty:\n",
    "            print(f\"⚠️ No matching activity for {filename}\")\n",
    "            continue\n",
    "\n",
    "        activity_id = match.iloc[0]['activity_id']\n",
    "        logs = parse_fit_file(filepath)\n",
    "\n",
    "        if not logs:\n",
    "            continue\n",
    "\n",
    "        for log in logs:\n",
    "            log['activity_id'] = activity_id\n",
    "\n",
    "        all_logs.extend(logs)\n",
    "\n",
    "    print(f\"✅ Parsed {len(all_logs)} total log entries from FIT files.\")\n",
    "    return pd.DataFrame(all_logs)\n",
    "\n",
    "def bulk_insert_fit_logs(df_logs):\n",
    "    \"\"\"Bulk insert fit logs with auto-incremented log_ids.\"\"\"\n",
    "    if df_logs.empty:\n",
    "        print(\"⚠️ No logs to insert.\")\n",
    "        return\n",
    "\n",
    "    next_id = get_next_id('fit_logs', 'log_id')\n",
    "    df_logs.insert(0, 'log_id', range(next_id, next_id + len(df_logs)))\n",
    "\n",
    "    df_logs.to_sql('fit_logs', con, if_exists='append', index=False)\n",
    "    print(f\"✅ Inserted {len(df_logs)} logs starting at ID {next_id}\")\n",
    "\n",
    "def test_query():\n",
    "    \"\"\"Query activities joined with fit_logs.\"\"\"\n",
    "    df = con.execute(\"\"\"\n",
    "        SELECT a.name, f.timestamp, f.field_name, f.field_value\n",
    "        FROM activities a\n",
    "        JOIN fit_logs f ON a.activity_id = f.activity_id\n",
    "        ORDER BY a.activity_id, f.timestamp\n",
    "        LIMIT 10\n",
    "    \"\"\").fetchdf()\n",
    "    print(\"\\n🎯 Sample joined data:\")\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68796acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tables created fresh.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c3/6yr501653kg68hk0nj1rjgkw0000gn/T/ipykernel_20327/4178603108.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df.to_sql('activities', con, if_exists='append', index=False)\n"
     ]
    },
    {
     "ename": "TransactionException",
     "evalue": "TransactionContext Error: cannot rollback - no transaction is active",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBinderException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/PYTHON_PROJECTS/Strava_Analysis/strava_env/lib/python3.9/site-packages/pandas/io/sql.py:2660\u001b[0m, in \u001b[0;36mSQLiteDatabase.run_transaction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2659\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2660\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m cur\n\u001b[1;32m   2661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon\u001b[38;5;241m.\u001b[39mcommit()\n",
      "File \u001b[0;32m~/Documents/PYTHON_PROJECTS/Strava_Analysis/strava_env/lib/python3.9/site-packages/pandas/io/sql.py:1119\u001b[0m, in \u001b[0;36mSQLTable.insert\u001b[0;34m(self, chunksize, method)\u001b[0m\n\u001b[1;32m   1118\u001b[0m chunk_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m(arr[start_i:end_i] \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m data_list))\n\u001b[0;32m-> 1119\u001b[0m num_inserted \u001b[38;5;241m=\u001b[39m \u001b[43mexec_insert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# GH 46891\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/PYTHON_PROJECTS/Strava_Analysis/strava_env/lib/python3.9/site-packages/pandas/io/sql.py:2547\u001b[0m, in \u001b[0;36mSQLiteTable._execute_insert\u001b[0;34m(self, conn, keys, data_iter)\u001b[0m\n\u001b[1;32m   2546\u001b[0m data_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data_iter)\n\u001b[0;32m-> 2547\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutemany\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert_statement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mrowcount\n",
      "\u001b[0;31mBinderException\u001b[0m: Binder Error: Table \"activities\" does not have a column with name \"Activity ID\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTransactionException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      4\u001b[0m     create_tables()\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mbulk_insert_activities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mACTIVITIES_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     logs_df \u001b[38;5;241m=\u001b[39m build_all_fit_logs()\n\u001b[1;32m      7\u001b[0m     bulk_insert_fit_logs(logs_df)\n",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m, in \u001b[0;36mbulk_insert_activities\u001b[0;34m(csv_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m next_id \u001b[38;5;241m=\u001b[39m get_next_id(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivities\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m df\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mrange\u001b[39m(next_id, next_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(df)))\n\u001b[0;32m---> 45\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mactivities\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Inserted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m activities starting at ID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/PYTHON_PROJECTS/Strava_Analysis/strava_env/lib/python3.9/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/PYTHON_PROJECTS/Strava_Analysis/strava_env/lib/python3.9/site-packages/pandas/core/generic.py:3087\u001b[0m, in \u001b[0;36mNDFrame.to_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;124;03mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[1;32m   2891\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;124;03m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   3085\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sql\n\u001b[0;32m-> 3087\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3088\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/PYTHON_PROJECTS/Strava_Analysis/strava_env/lib/python3.9/site-packages/pandas/io/sql.py:842\u001b[0m, in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument should be either a Series or a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    839\u001b[0m     )\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con, schema\u001b[38;5;241m=\u001b[39mschema, need_transaction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m--> 842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/PYTHON_PROJECTS/Strava_Analysis/strava_env/lib/python3.9/site-packages/pandas/io/sql.py:2851\u001b[0m, in \u001b[0;36mSQLiteDatabase.to_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[1;32m   2841\u001b[0m table \u001b[38;5;241m=\u001b[39m SQLiteTable(\n\u001b[1;32m   2842\u001b[0m     name,\n\u001b[1;32m   2843\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2848\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   2849\u001b[0m )\n\u001b[1;32m   2850\u001b[0m table\u001b[38;5;241m.\u001b[39mcreate()\n\u001b[0;32m-> 2851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/PYTHON_PROJECTS/Strava_Analysis/strava_env/lib/python3.9/site-packages/pandas/io/sql.py:1125\u001b[0m, in \u001b[0;36mSQLTable.insert\u001b[0;34m(self, chunksize, method)\u001b[0m\n\u001b[1;32m   1123\u001b[0m                 total_inserted \u001b[38;5;241m=\u001b[39m num_inserted\n\u001b[1;32m   1124\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1125\u001b[0m                 total_inserted \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_inserted\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_inserted\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    133\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m()\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/Documents/PYTHON_PROJECTS/Strava_Analysis/strava_env/lib/python3.9/site-packages/pandas/io/sql.py:2663\u001b[0m, in \u001b[0;36mSQLiteDatabase.run_transaction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[1;32m   2662\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 2663\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2664\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   2665\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mTransactionException\u001b[0m: TransactionContext Error: cannot rollback - no transaction is active"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- MAIN FLOW ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_tables()\n",
    "    bulk_insert_activities(ACTIVITIES_FILE)\n",
    "    logs_df = build_all_fit_logs()\n",
    "    bulk_insert_fit_logs(logs_df)\n",
    "    test_query()\n",
    "    print(\"\\n🏁 All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7570382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b5a80f",
   "metadata": {},
   "source": [
    "# Basic DuckDB example -- Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba31806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to a DuckDB database file (it will create it if it doesn't exist)\n",
    "con = duckdb.connect('test_project.duckdb')\n",
    "\n",
    "# Drop tables if they exist (to start fresh)\n",
    "con.execute(\"DROP TABLE IF EXISTS activities\")\n",
    "con.execute(\"DROP TABLE IF EXISTS fit_logs\")\n",
    "\n",
    "# Create activities table (MINIMAL, NO constraints)\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE activities (\n",
    "        activity_id BIGINT,\n",
    "        name TEXT,\n",
    "        date TIMESTAMP,\n",
    "        distance DOUBLE,\n",
    "        duration DOUBLE,\n",
    "        fit_filename TEXT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Create fit_logs table (also MINIMAL)\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE fit_logs (\n",
    "        log_id BIGINT,\n",
    "        activity_id BIGINT,\n",
    "        timestamp TIMESTAMP,\n",
    "        field_name TEXT,\n",
    "        field_value DOUBLE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Tables created successfully.\")\n",
    "\n",
    "# Insert one fake record into activities\n",
    "con.execute(\"\"\"\n",
    "    INSERT INTO activities (activity_id, name, date, distance, duration, fit_filename)\n",
    "    VALUES (1, 'Morning Run', NOW(), 5.0, 30.0, 'run01.fit')\n",
    "\"\"\")\n",
    "\n",
    "# Insert one fake record into fit_logs\n",
    "con.execute(\"\"\"\n",
    "    INSERT INTO fit_logs (log_id, activity_id, timestamp, field_name, field_value)\n",
    "    VALUES (1, 1, NOW(), 'heart_rate', 145)\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Inserted example data.\")\n",
    "\n",
    "# Query and print\n",
    "df = con.execute(\"\"\"\n",
    "    SELECT a.name, f.timestamp, f.field_name, f.field_value\n",
    "    FROM activities a\n",
    "    JOIN fit_logs f ON a.activity_id = f.activity_id\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"\\n🎯 Query result:\")\n",
    "print(df)\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5ad82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0031a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7672df8",
   "metadata": {},
   "source": [
    "# Early DuckDB Example - doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad128b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import duckdb\n",
    "from fitparse import FitFile\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "DB_FILE = 'strave_test_db.duckdb'\n",
    "ACTIVITIES_FILE = 'strava_data_dumps/STRAVA+export_8029714/activities.csv'\n",
    "FIT_FILES_FOLDER = 'strava_data_dumps/STRAVA+export_8029714/activities/'\n",
    "\n",
    "IMPORTANT_FIELDS = ['heart_rate', 'cadence', 'speed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70589ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect(DB_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d84895",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def create_tables():\n",
    "    \"\"\"Create activities and fit_logs tables (fully DuckDB compatible).\"\"\"\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS activities (\n",
    "            activity_id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "            name TEXT,\n",
    "            date TIMESTAMP,\n",
    "            distance DOUBLE,\n",
    "            duration DOUBLE,\n",
    "            fit_filename TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS fit_logs (\n",
    "            log_id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "            activity_id BIGINT,\n",
    "            timestamp TIMESTAMP,\n",
    "            field_name TEXT,\n",
    "            field_value DOUBLE\n",
    "        )\n",
    "    \"\"\")\n",
    "    print(\"✅ Tables created (truly DuckDB-safe).\")\n",
    "\n",
    "\n",
    "def load_activities():\n",
    "    \"\"\"Load activities.csv into the database.\"\"\"\n",
    "    df = pd.read_csv(ACTIVITIES_FILE)\n",
    "\n",
    "    if 'fit_filename' not in df.columns:\n",
    "        raise ValueError(\"activities.csv must have a 'fit_filename' column.\")\n",
    "\n",
    "    df.to_sql('activities', con, if_exists='append', index=False)\n",
    "    print(f\"✅ Inserted {len(df)} activities.\")\n",
    "\n",
    "def parse_fit_file(filepath):\n",
    "    \"\"\"Parse a .fit file into structured log entries.\"\"\"\n",
    "    try:\n",
    "        fitfile = FitFile(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to parse {filepath}: {e}\")\n",
    "        return []\n",
    "\n",
    "    entries = []\n",
    "    for record in fitfile.get_messages('record'):\n",
    "        timestamp = None\n",
    "        metrics = []\n",
    "\n",
    "        for field in record:\n",
    "            if field.name == 'timestamp':\n",
    "                timestamp = field.value\n",
    "            elif field.name in IMPORTANT_FIELDS and field.value is not None:\n",
    "                metrics.append((field.name, field.value))\n",
    "\n",
    "        if timestamp:\n",
    "            for field_name, field_value in metrics:\n",
    "                entries.append({\n",
    "                    'timestamp': timestamp,\n",
    "                    'field_name': field_name,\n",
    "                    'field_value': field_value\n",
    "                })\n",
    "\n",
    "    return entries\n",
    "\n",
    "def find_activity_id(filename, activities_df):\n",
    "    \"\"\"Match .fit filename to activity_id.\"\"\"\n",
    "    match = activities_df[activities_df['fit_filename'] == filename]\n",
    "    if not match.empty:\n",
    "        return match.iloc[0]['activity_id']\n",
    "    else:\n",
    "        print(f\"⚠️ No matching activity found for {filename}\")\n",
    "        return None\n",
    "\n",
    "def insert_fit_logs():\n",
    "    \"\"\"Loop over fit files, parse, and insert logs into the database.\"\"\"\n",
    "    activities_df = pd.read_sql(\"SELECT activity_id, fit_filename FROM activities\", con)\n",
    "\n",
    "    fit_files = glob.glob(os.path.join(FIT_FILES_FOLDER, '*.fit'))\n",
    "\n",
    "    total_logs_inserted = 0\n",
    "\n",
    "    for filepath in fit_files:\n",
    "        filename = os.path.basename(filepath)\n",
    "        activity_id = find_activity_id(filename, activities_df)\n",
    "\n",
    "        if activity_id is None:\n",
    "            continue\n",
    "\n",
    "        logs = parse_fit_file(filepath)\n",
    "\n",
    "        if not logs:\n",
    "            continue\n",
    "\n",
    "        for log in logs:\n",
    "            log['activity_id'] = activity_id\n",
    "\n",
    "        logs_df = pd.DataFrame(logs)\n",
    "\n",
    "        logs_df.to_sql('fit_logs', con, if_exists='append', index=False)\n",
    "        total_logs_inserted += len(logs_df)\n",
    "        print(f\"✅ Inserted {len(logs_df)} logs from {filename}\")\n",
    "\n",
    "    print(f\"\\n✅ Total logs inserted: {total_logs_inserted}\")\n",
    "\n",
    "def test_query():\n",
    "    \"\"\"Test if data is in database.\"\"\"\n",
    "    df = pd.read_sql(\"\"\"\n",
    "        SELECT a.name, f.timestamp, f.field_name, f.field_value\n",
    "        FROM activities a\n",
    "        JOIN fit_logs f ON a.activity_id = f.activity_id\n",
    "        ORDER BY a.activity_id, f.timestamp\n",
    "        LIMIT 10\n",
    "    \"\"\", con)\n",
    "    print(\"\\n🎯 Sample joined data:\")\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b23b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- MAIN FLOW ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_tables()\n",
    "    load_activities()\n",
    "    insert_fit_logs()\n",
    "    test_query()\n",
    "    print(\"\\n🏁 All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdcb2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "strava_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
